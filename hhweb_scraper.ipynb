{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting off the scraper from the main project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.error import HTTPError\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "\n",
    "# Tor stuff, this might not all be needed\n",
    "# TODO Clean it up later\n",
    "import stem\n",
    "import requests\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "from stem.util import term\n",
    "import socks as pysocks\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This can only be done once\n",
    "controller = Controller.from_port(port = 9051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting from 212.47.246.21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def new_ident():\n",
    "    controller.authenticate()\n",
    "    controller.signal(Signal.NEWNYM)\n",
    "    pysocks.setdefaultproxy(pysocks.PROXY_TYPE_SOCKS5 , \"127.0.0.1\", 9050, True)\n",
    "    socket.socket = pysocks.socksocket\n",
    "    print('Connecting from {}'.format(requests.get('http://icanhazip.com').text))\n",
    "    time.sleep(4)\n",
    "\n",
    "\n",
    "for i in range(0,1):    \n",
    "    new_ident()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting from 212.47.246.21\n",
      "\n",
      "493 sub pages found.\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#thereis, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#losos, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#nocomp2, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#soultape, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#nocomp3, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#soultape2, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#soul3, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#young, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#summertime, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#remix, it has a #\n",
      "Skipping http://ohhla.com/YFA_fab.htmlYFA_fab.html#misc, it has a #\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlhttp://www.amazon.com/gp/product/B00005O6DS?ie=UTF8&tag=theorihiphopl-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B00005O6DS\n",
      "ly_lnk is not valid.  Skipping it. HTTP Error 404: Not Found\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/clickand.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/keepinit.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/young_n.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/getright.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/ride_for.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/one_day.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/trade_it.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/rightnow.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/take_you.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/getsmart.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/cantdeny.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/mabeeasy.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/we_dont.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/bad_guy.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/gotta_be.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/ghetto_f/want_it.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/anonymous/fabolous/ghetto_f/\n",
      "ly_lnk is not valid.  Skipping it. must be str, not NoneType\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlhttp://www.amazon.com/gp/product/B00006LHYQ?ie=UTF8&tag=theorihiphopl-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B00006LHYQ\n",
      "ly_lnk is not valid.  Skipping it. HTTP Error 404: Not Found\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/not_give.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/damn.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/call_me.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/cant_let.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/badbitch.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/wouldnt.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/things.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/sick.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/this_is.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/into_you.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/change.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/respect.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/forgive.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/neverdup.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/my_life.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/throwbck.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/kigremix.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/street/tradept2.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/anonymous/fabolous/street/\n",
      "ly_lnk is not valid.  Skipping it. must be str, not NoneType\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlhttp://www.amazon.com/gp/product/B0000DBK7D?ie=UTF8&tag=theorihiphopl-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=B0000DBK7D\n",
      "ly_lnk is not valid.  Skipping it. HTTP Error 404: Not Found\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/mstreet2/niggaz.fab.txt\n",
      "Using alternate workflow for artist page for: http://ohhla.com/YFA_fab.htmlanonymous/fabolous/mstreet2/now_what.fab.txt\n"
     ]
    }
   ],
   "source": [
    "# Drilling down through the directory structure with the scraper\n",
    "def pull_lyrics():\n",
    "    new_ident()\n",
    "    \n",
    "    u = urllib.request.urlopen(\"http://ohhla.com/all_two.html\")\n",
    "    soup = BeautifulSoup(u, 'html.parser')\n",
    "    submitters = soup.select('#leftmain table pre a')\n",
    "    sub_pages = []\n",
    "    for link in submitters: \n",
    "        rel = (link.get('href'))\n",
    "        if rel:\n",
    "            sub_pages.append('http://ohhla.com/' + rel)\n",
    "    print(len(sub_pages), 'sub pages found.')\n",
    "    batch_step = 1 \n",
    "    for i in range(0, len(sub_pages), batch_step):\n",
    "        batch = sub_pages[i:i+batch_step]\n",
    "        lyric_text(batch)\n",
    "        print('{} OF {} COMPLETED'.format((1+i)*batch_step, len(sub_pages)))\n",
    "\n",
    "# This didn't solve the issue, but will be useful once we get Tor working\n",
    "# Break into a seperate function here to avoid time outs\n",
    "def lyric_text(passed_pages):\n",
    "    # Made passed pages a set to cancel out the duplication for artist pages\n",
    "    # TODO, see if this works\n",
    "    \n",
    "    for artist in set(passed_pages):\n",
    "        if artist:\n",
    "            try:\n",
    "                sub_u = urllib.request.urlopen(artist) \n",
    "                sub_soup = (BeautifulSoup(sub_u, 'html.parser'))\n",
    "                lyrics_pages = sub_soup.select('body table tr a')\n",
    "                for lyrics_page in lyrics_pages[5:]:\n",
    "                    if lyrics_page: \n",
    "                        try:\n",
    "#                             print('Artist:{}, Lyrics page:{}'.format(artist, lyrics_page))\n",
    "                            ly_lnk = artist+lyrics_page.get('href')\n",
    "                            if '#' not in ly_lnk: # Lyrics pages with # are just links to anchors on the same page\n",
    "                                ly_u = urllib.request.urlopen(ly_lnk)\n",
    "                                ly_soup = BeautifulSoup(ly_u, 'html.parser')\n",
    "                                ly_txts = ly_soup.findAll(href=re.compile(\"\\.txt$\"))             \n",
    "                                for ly_txt in ly_txts:\n",
    "\n",
    "                                        attempt_lyrics_save(ly_txt, ly_lnk)\n",
    "                            else:\n",
    "                                print('Skipping {}, it has a #'.format(ly_lnk))\n",
    "\n",
    "                        # Some of the artists are organized in an artist page, or we could need a new identy\n",
    "                        # Artist page is more likely so we check for it first\n",
    "                        except (TypeError, HTTPError) as e:\n",
    "                            if '#' not in ly_lnk: # Lyrics pages with # are just links to anchors on the same page\n",
    "                                try:\n",
    "                                    # Switch identity in case it was part of the issue\n",
    "                                    print('Using alternate workflow for artist page for: {}'.format(ly_lnk))\n",
    "                                    ly_lnk = 'http://ohhla.com/'+lyrics_page.get('href')\n",
    "                                    # Right now this is just checking that it does open\n",
    "                                    ly_u = urllib.request.urlopen(ly_lnk)\n",
    "                                    ly_soup = BeautifulSoup(ly_u, 'html.parser')\n",
    "                                    ly_txt = re.search('[\\w\\.]*txt', ly_lnk).group(0)\n",
    "                                    ly_lnk = ly_lnk[0:-len(ly_txt)]\n",
    "#                                     print('Attempting lyrics save ly_txt:{}, ly_lnk:{}'.format(ly_txt, ly_lnk))\n",
    "                                    attempt_lyrics_save(ly_txt, ly_lnk, direct_link=True)\n",
    "\n",
    "                                except (TypeError, HTTPError) as e: \n",
    "                                    print('ly_lnk' + ' is not valid.  Skipping it.', e)      \n",
    "                        # At this point we have a new identity, so check if that was the problem \n",
    "                        # This should only get hit if the format was normal *but* we exceeded the rate limit\n",
    "                        except (TypeError, HTTPError):\n",
    "                            print('Trying base format with new identity')\n",
    "                            ly_lnk = artist+lyrics_page.get('href')\n",
    "                            ly_u = urllib.request.urlopen(ly_lnk)\n",
    "                            ly_soup = BeautifulSoup(ly_u, 'html.parser')\n",
    "                            ly_txts = ly_soup.findAll(href=re.compile(\"\\.txt$\"))             \n",
    "                            for ly_txt in ly_txts:\n",
    "\n",
    "                                    attempt_lyrics_save(ly_txt, ly_lnk)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print('A non TypeError has occured. ', e)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Artist: {}\\n raised {}'.format(artist, e))\n",
    "                print(traceback.print_exc())\n",
    "            time.sleep(1+(random.random()*2))\n",
    "\n",
    "\n",
    "def attempt_lyrics_save(page_in, ly_lnk_in, final_run=False, direct_link=False):\n",
    "    if direct_link is False:\n",
    "        page = page_in.get('href')\n",
    "    else:\n",
    "        page = page_in\n",
    "    lyrics = urllib.request.urlopen(ly_lnk_in + page)\n",
    "    lyrics_soup = BeautifulSoup(lyrics, 'html.parser')\n",
    "    lyrics_cln = lyrics_soup.select('body div pre')\n",
    "    lyrics_cln = str(lyrics_cln)[7:-7] # Stripping out the pre tags (I know it's ugly)\n",
    "    filename = page.rsplit('/', 1)[-1]    \n",
    "    \n",
    "    if len(lyrics_cln) > 0:\n",
    "        with open('lyrics/' + filename, 'w+') as out:\n",
    "            out.write(str(lyrics_cln))\n",
    "        if final_run==True:\n",
    "            print('After identity switching, the lyrics length is {}'.format(len(lyrics_cln)))\n",
    "\n",
    "    elif final_run==True:\n",
    "        # We're being denied now\n",
    "        print('We are getting a blank lyrics page', ly_lnk_in+page) \n",
    "        new_ident()\n",
    "        attempt_lyrics_save(page_in, ly_lnk_in, final_run=True)\n",
    "pull_lyrics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
